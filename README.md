
---

# ğŸš€ **Data-Efficient Multi-Property Prediction of Crystals Using Active Learning & GNNs**

### *Group 14 â€“ IIT Madras (MLMS Course Project)*

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-Geometric-orange.svg)
![Active Learning](https://img.shields.io/badge/Active%20Learning-MC%20Dropout%20%2F%20Ensembles-green.svg)
![License](https://img.shields.io/badge/License-Academic-blue)

---

## ğŸ“Œ **Project Summary**

This repository contains the complete implementation, datasets, experiments, and results for the project:

> **â€œData-Efficient Multi-Property Prediction of Crystals Using Active Learning and Graph Neural Networksâ€**

This work benchmarks **4 GNN architectures** and **4 learning strategies** on predicting:

* **Formation Energy (FE)**
* **Band Gap (BG)**
  simultaneously (multi-target regression)

using both **filtered** (application-specific) and **unfiltered** (raw, heterogeneous) datasets from the **Materials Project**.

### â­ Key Highlights

* 4 GNN models benchmarked: **Custom GNN (GCN), SchNet, CGCNN, MEGNet**
* 4 learning strategies:

  * **Traditional Supervised**
  * **Random Sampling**
  * **Active Learning (MC Dropout)**
  * **Active Learning (Deep Ensembles)**
* 2 datasets:

  * **6,996 curated materials (filtered)**
  * **50,000 raw materials (unfiltered)**
* Extensive comparisons across:

  * Data efficiency
  * Architectures
  * AL strategies
  * Dataset quality

All results, plots, and trained models are included.

---

# ğŸ“„ **Project Report (PDF)**

The final 10-page research report submitted for the course:
ğŸ‘‰ **`Report.pdf`**

---

# ğŸ“ **Repository Structure**

You can add this in GitHub for clarity.

```
ğŸ“¦ scripts
â”‚
â”œâ”€â”€ ğŸ“ Filtered/                    # Experiments on curated dataset
â”‚   â”œâ”€â”€ main.ipynb
â”‚   â”œâ”€â”€ ActiveLearning/
â”‚   â”œâ”€â”€ TraditionalLearning/
â”‚   â”œâ”€â”€ RandomSampling/
â”‚   â”œâ”€â”€ EnsembleActiveLearning/
â”‚   â”œâ”€â”€ Datasets/
â”‚   â””â”€â”€ Results/
â”‚
â””â”€â”€ ğŸ“ Unfiltered/                  # Raw dataset experiments (50k materials)
    â”œâ”€â”€ main.ipynb
    â”œâ”€â”€ ActiveLearning/
    â”œâ”€â”€ TraditionalLearning/
    â”œâ”€â”€ RandomSampling/
    â”œâ”€â”€ EnsembleActiveLearning/
    â”œâ”€â”€ Datasets/
    â””â”€â”€ Results/
```

Each subfolder contains:

* `.pth` model checkpoints
* `.csv` training histories
* `.pt` graph datasets
* Visualizations (`.png`)
* Queried sample IDs
* Ensemble members
* Scripts for training/evaluation

---

# ğŸ§  **Research Motivation**

DFT simulations are extremely expensive (hoursâ€“days per structure).
This project answers the key question:

### **Can we reduce required training data by actively selecting only the most informative crystal structures?**

We benchmark:

* Different **uncertainty estimation methods**
* Different **GNN architectures**
* Both **filtered & unfiltered** datasets

---

# ğŸ”¬ **Methodology Overview**

## 1. Dataset Acquisition & Curation

* 50,000 materials pulled from **Materials Project API**
* Filtered dataset created through **application-driven filtering**:

| Property         | Range (Kept)        | Motivation               |
| ---------------- | ------------------- | ------------------------ |
| Formation Energy | -3.5 to 0.5 eV/atom | Thermodynamic realism    |
| Band Gap         | 0.01 to 1.2 eV      | Practical semiconductors |

Filtering yields a **coherent, physically motivated** dataset of **6,996 crystals**.

## 2. Graph Representation

Each crystal â†’ graph:

* Nodes = atoms (atomic number embedding)
* Edges = neighbors within **5 Ã…**
* Edge features = interatomic distance

Representation built with **PyTorch Geometric**.

## 3. Models Implemented

### ğŸ”µ **Custom GNN (GCN-based)**

* 3 Ã— GCNConv + ReLU
* Global Mean Pooling
* 2-layer MLP head
* Output: `[FE, BG]`

### ğŸŸ£ **SchNet**

* Continuous-filter convolutions
* RBF-expanded distances
* Shifted softplus activation

### ğŸŸ¢ **CGCNN**

* Gated message passing
* Crystal-specific nodeâ€“edge concatenation
* Sum pooling

### ğŸ”´ **MEGNet (GAT-based adaptation)**

* GATv2 attention
* Node/edge encoders
* Sum pooling

---

# ğŸ” **Learning Strategies Implemented**

### 1ï¸âƒ£ **Traditional Learning**

Fixed training set of 1,200 samples.

### 2ï¸âƒ£ **Random Sampling**

Iterative training:

* Start with 200 samples
* Add 100 random samples Ã— 10 cycles

### 3ï¸âƒ£ **Active Learning (MC Dropout)**

* Dropout active during inference
* 20 stochastic passes per sample
* Query highest-variance points

### 4ï¸âƒ£ **Active Learning (Deep Ensembles)**

* 5 independently trained models
* Uncertainty = prediction variance across ensemble
* Best performance in our results

---

# ğŸ“Š **Key Results (Summary)**

### â­ 1. Active Learning is Highly Effective

* Requires **33â€“50% less data** to reach same performance
* Ensemble AL performs best in most settings

### â­ 2. Custom GNN Outperforms Complex Models

* Most stable and accurate model across all scenarios
* SchNet/CGCNN exhibit catastrophic divergence on unfiltered data

### â­ 3. Dataset Quality Matters Greatly

* Filtered data: stable & consistent learning
* Unfiltered data:

  * SchNet explodes to **10Â¹Â³ loss**
  * CGCNN unstable
  * Only Custom GNN + Ensemble AL survives reliably

### â­ 4. Active Learning Selects Informative, Diverse Materials

It explores:

* Extremes of FE/BG
* Structurally diverse crystals
* High-value candidates for DFT follow-up

---

# ğŸ“ˆ **Example Images (Placeholders)**

When you upload, GitHub will render them:


![Filtered Dataset Distribution](scripts/Filtered/Datasets/target_properties_distribution.png)
![Learning Curves](scripts/Filtered/Results/final_comparison_fe_all_strategies.png)
![Parity Plot](scripts/Filtered/Results/parity_plots_all_models_all_strategies_formation_energy.png)


---

# âš™ï¸ **Installation**

### 1. Create environment:

```bash
conda create -n mlms python=3.10
conda activate mlms
```

### 2. Install dependencies:

```bash
pip install -r requirements.txt
```

If you donâ€™t have the file:

```bash
pip install torch torchvision torchaudio torch-geometric pandas numpy scikit-learn tqdm matplotlib pymatgen joblib
```

---

# â–¶ï¸ **How to Run the Project**

The easiest method:

## **Run Jupyter Notebooks**

```
Filtered/main.ipynb
Unfiltered/main.ipynb
```

All experiments (training loops, active learning, visualizations) are automated.



---

# ğŸ“ **Outputs Produced**

By default, results include:

* Model weights: `*.pth`
* Active learning queried IDs
* Histories: `.csv`
* Parity plots & comparison charts: `.png`
* FE/BG predictions for each model
* Distribution visualizations
* Ensemble disagreement curves

Stored in:

```
Filtered/Results/
Unfiltered/Results/
```

---

# âœ¨ **Author Contributions**

This project was completed **individually**:

**Chamakura Sai Kumar (ED25D402)**

* Proposed research idea
* Coded all 4 GNN models
* Implemented Active Learning pipelines
* Wrote training/evaluation engines
* Curated Materials Project dataset
* Performed all experiments
* Generated all plots
* Wrote the final report (MLMS_IITM.pdf)

---

# ğŸ“š **References**

All citations from the report are included in the PDF.

---

# â¤ï¸ **Acknowledgements**

* Materials Project API
* PyTorch, PyTorch Geometric
* pymatgen
* IIT Madras â€” MLMS Course

---

# ğŸ“¬ **Contact**

For questions or collaboration:

ğŸ“§ **[saichamakura5215@gmail.com](mailto:saichamakura5215@gmail.com)**

---
